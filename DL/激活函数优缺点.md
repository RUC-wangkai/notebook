详情可以参考这篇 [文章](https://zhuanlan.zhihu.com/p/25110450)

1. sigmoid
   
   缺点: 容易出现梯度消失
        zero-centered: 输出值恒>0 可能会导致模型训练收敛速度变慢
        幂运算相对耗时

2. tanh

    相对于sigmoid他解决了zero-centered的输出问题,但是梯度消失和幂运算问题仍然存在

3. Relu
   
   relu解决了梯度消失的问题,计算速度快,收敛速度远快于sigmoid和tanh
   但是relu的输出不是zero-centered,并且会存在Dead Relu Problem 也就是指某些神经元可能永远不会被激活导致相应参数永远不能被更新